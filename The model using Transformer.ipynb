{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a67b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import torch # PyTorch\n",
    "from torch import nn # Neural Network \n",
    "from torch import Tensor\n",
    "from torch.nn import Transformer\n",
    "from torch.utils.data import DataLoader, Dataset # Wrap an iterable around datasets\n",
    "from torch.nn.utils.rnn import pad_sequence # Pad to make variable length sequence to same length\n",
    "\n",
    "# Transform input's features and labels to suitable tensors\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "# Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "# Random\n",
    "import random\n",
    "from random import seed\n",
    "from random import randint\n",
    "# Optimizer and loss function\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f3c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the Training_data.txt\n",
    "# getitem returns a tuple of sequence as tensor, label, length of sequence\n",
    "class CigarDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # data loading\n",
    "        data = np.loadtxt('./train60k.txt', delimiter=\"/t\", dtype=str)\n",
    "        self.ind = data[:,0]\n",
    "        self.seq = data[:,1]\n",
    "        self.label = data[:,2]\n",
    "        self.over = data[:,3]\n",
    "        self.n_samples = data.shape[0]\n",
    "    def __getitem__(self,index):\n",
    "        # dataset[0]\n",
    "        fx = list(self.seq[index])\n",
    "        for i in range(len(fx)):\n",
    "            if fx[i] == 'A':\n",
    "                fx[i] = 1 \n",
    "            if fx[i] == 'T':\n",
    "                fx[i] = 2 \n",
    "            if fx[i] == 'G':\n",
    "                fx[i] = 3 \n",
    "            if fx[i] == 'C':\n",
    "                fx[i] = 4 \n",
    "        # Type string\n",
    "        #label encode target and ensure the values are floats\n",
    "        self.label = LabelEncoder().fit_transform(self.label)\n",
    "        \n",
    "        #bac_key = ['lm','ef','ec','pa','bs','sa','se']\n",
    "        #for i in range(len(bac_key)):\n",
    "        #    if bac_key[i] == self.label[index]:\n",
    "        #        label = i\n",
    "        \n",
    "        index1 = self.ind[index] # Index or name of the x1. This is a string.\n",
    "        \n",
    "        overlap = self.over[index] # List of overlapping sequences\n",
    "        overlap = overlap.replace(\"[\", \"\")\n",
    "        overlap = overlap.replace(\"]\", \"\")\n",
    "        overlap = overlap.split(',')\n",
    "        overlap = [int(i) for i in overlap] # Because of the format of the input file, need some modification here.\n",
    "        index2 = random.sample(overlap,1)\n",
    "        \n",
    "        this = 0\n",
    "        for i in range(len(self.ind)):\n",
    "            if int(self.ind[i]) == index2[0]:\n",
    "                this = i\n",
    "                break\n",
    "        that = self.seq[this]\n",
    "        \n",
    "        sample = self.seq[index], self.label[index], len(fx), that, len(that)\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bfcfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CigarDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589eb160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(seq):\n",
    "    # Input a DNA of type string\n",
    "    # Output a tensor\n",
    "    # creating instance of labelencoder\n",
    "    labelencoder = LabelEncoder()\n",
    "    # Assigning numerical values and storing in another column\n",
    "    seq = labelencoder.fit_transform(seq)\n",
    "    \n",
    "    # creating instance of one-hot-encoder\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = seq.reshape(len(seq),1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    \n",
    "    out = torch.tensor(onehot_encoded)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4af9866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "       data: is a list of tuples with (seq, label, length, ind1, ind2)\n",
    "             where 'seq' is a list of arbitrary length\n",
    "    \"\"\"\n",
    "    # each data is a batch of size 64 sequences\n",
    "    seq, labels, len_seq, overlap, len_overlap  = zip(*data)\n",
    "    \n",
    "    # seq is a list of form ATCG\n",
    "    # Use embed here\n",
    "    # Create a list of tensor\n",
    "    seq = [embed(list(sequ)) for sequ in seq]\n",
    "    overlap = [embed(list(over)) for over in overlap]\n",
    "    \n",
    "    features = pad_sequence(seq, batch_first = True,padding_value= 2.0)\n",
    "    \n",
    "    overlaps = pad_sequence(overlap, batch_first = True,padding_value= 2.0)\n",
    "    return features, labels, len_seq, overlaps, len_overlap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43874140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset\n",
    "# Create dataloader.\n",
    "batch_size = 64\n",
    "train_dl = DataLoader(data, batch_size=batch_size, shuffle = True, collate_fn=collate_fn, drop_last = True)\n",
    "\n",
    "for src, label , leng_src, overlap, leng_over in train_dl:\n",
    "    print(\"Shape of feature: \", src.shape,src.dtype)\n",
    "    print(\"label = \",len(label))\n",
    "    print(\"length = \",leng_src)\n",
    "    print(\"Overlap:\",overlap)\n",
    "    print(\"Length overlap:\",leng_over)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487b54ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model with forward process\n",
    "class OurModel(nn.Module):\n",
    "    def __init__(self, d_model:int, nhead:int, num_layer:int):\n",
    "        super(OurModel, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layer)\n",
    "        \n",
    "    def forward(self, src: Tensor, mask: Tensor , src_key_padding_mask: Tensor) -> Tensor:\n",
    "        output = self.transformer_encoder(src, mask, src_key_padding_mask)\n",
    "        return F.normalize(output,dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2668fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE = torch.device('cpu')\n",
    "model = OurModel(d_model = 512, nhead=8, num_layer=6).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8e55aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch, (X,label,leng_X,Overlap,leng_O) in enumerate(train_dl):\n",
    "        print('Seq after one-hot encoding: ',X.shape)\n",
    "        X = X.unfold(1,128,128) # Want to use d_model = 512, and currently we have 4. So unfolding use 512/4=128\n",
    "        X = X.reshape([64,-1,512])\n",
    "        X = X.type(torch.float32) # The TransformerEncoder uses float32.\n",
    "        print('Seq after unfold+reshape encoding: ',X.shape)\n",
    "        \n",
    "        cls = nn.Parameter(torch.randn(64, 1, 512))\n",
    "        src1 = torch.cat((cls,X), dim=1) \n",
    "        \n",
    "        print('Overlap after one-hot encoding: ',Overlap.shape)\n",
    "        Overlap = Overlap.unfold(1,128,128) # Want to use d_model = 512, and currently we have 4. So unfolding use 512/4=128\n",
    "        Overlap = Overlap.reshape([64,-1,512])\n",
    "        Overlap = Overlap.type(torch.float32) # The TransformerEncoder uses float32.\n",
    "        src2 = torch.cat((cls,Overlap), dim=1) \n",
    "        \n",
    "        pred1 = model(src1, mask = None, src_key_padding_mask = None)\n",
    "        print(pred1.shape)\n",
    "        \n",
    "        pred2 = model(src2, mask = None, src_key_padding_mask = None)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e10f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pred1[:,0,:]\n",
    "print('Prediction 1: ',t1)\n",
    "t2 = pred2[:,0,:]\n",
    "print('Prediction 2: ',t2)\n",
    "#### Do layer normalization\n",
    "temperature = 0.1\n",
    "out = torch.cat([t1, t2], dim=0)\n",
    "n_samples = len(out)\n",
    "\n",
    "cov = torch.mm(out, out.t().contiguous())\n",
    "sim = torch.exp(cov / temperature)\n",
    "\n",
    "mask = ~torch.eye(n_samples).bool()\n",
    "neg = sim.masked_select(mask).view(n_samples, -1).sum(dim=-1)\n",
    "\n",
    "print(torch.sum(t1*t2,dim=-1))\n",
    "pos = torch.exp(torch.sum(t1 * t2, dim=-1) / temperature)\n",
    "pos = torch.cat([pos, pos], dim=0)\n",
    "print(neg)\n",
    "loss = -torch.log(pos / neg).mean()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a628271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nt_xent_loss(out_1, out_2, temperature):\n",
    "    \"\"\"Loss used in SimCLR.\"\"\"\n",
    "    out = torch.cat([out_1, out_2], dim=0)\n",
    "    n_samples = len(out)\n",
    "\n",
    "    # Full similarity matrix\n",
    "    cov = torch.mm(out, out.t().contiguous())\n",
    "    sim = torch.exp(cov / temperature)\n",
    "\n",
    "    # Negative similarity\n",
    "    mask = ~torch.eye(n_samples).bool()\n",
    "    neg = sim.masked_select(mask).view(n_samples, -1).sum(dim=-1)\n",
    "\n",
    "    # Positive similarity :\n",
    "    pos = torch.exp(torch.sum(out_1 * out_2, dim=-1) / temperature)\n",
    "    pos = torch.cat([pos, pos], dim=0)\n",
    "    loss = -torch.log(pos / neg).mean()\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
